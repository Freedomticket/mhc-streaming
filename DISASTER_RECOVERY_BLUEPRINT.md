# MHC STREAMING - DISASTER RECOVERY & CONTINUITY BLUEPRINT

**Status**: ACTIVE AND MANDATORY
**Last Updated**: December 13, 2025
**Purpose**: Ensure MHC survives any infrastructure failure or Big Tech shutdown

---

## üî• THE SCENARIO

**Worst-Case Situation**:
- Primary data center goes down
- Internet connectivity lost for 24+ hours
- Stripe becomes unavailable
- AWS, Google Cloud, Azure all offline
- DNS servers not responding
- All external CDNs unreachable

**MHC Must Still**:
- ‚úÖ Continue streaming
- ‚úÖ Continue taking payments
- ‚úÖ Continue serving content
- ‚úÖ Continue processing transactions
- ‚úÖ Maintain artist royalties
- ‚úÖ Keep users authenticated
- ‚úÖ Preserve all data

---

## üìä INFRASTRUCTURE LAYER STRATEGY

### Layer 1: Primary Production
```
- Main Datacenter: Bare Metal (not cloud)
- OS: Ubuntu Server 22.04 LTS
- Services: Express.js + PostgreSQL + Redis + MinIO
- Monitoring: Prometheus + Grafana
- Redundancy: RAID 6 SSD storage
```

### Layer 2: Warm Standby (Automatic Failover)
```
- Secondary VPS: DigitalOcean / Hetzner (not AWS)
- Replication: PostgreSQL Streaming Replication
- Keep: 1-minute lag behind primary
- Promotion: DNS failover on primary outage
```

### Layer 3: Cold Storage (Deep Backups)
```
- Location: Physical datacenter (ZFS snapshots)
- Frequency: Hourly snapshots + Daily archives
- Retention: 90-day rolling window
- Format: WAL archives (point-in-time recovery)
```

### Layer 4: Satellite/Mesh (Last Resort)
```
- Offline Mode: Content cached locally on mobile
- Peer Mesh: Devices share content P2P
- Satellite: Starlink/OneWeb receive-only fallback
- Works: Even without internet backbone
```

---

## ‚úÖ AUTOMATIC BACKUP ARCHITECTURE

### Database Backups (PostgreSQL)

```bash
# Cron: Every hour at :00
0 * * * * pg_dump -Fc mhc > /backups/mhc_hourly_$(date +\%Y\%m\%d_\%H).dump

# Cron: Every day at 2 AM UTC
0 2 * * * pg_dump -Fc mhc > /backups/mhc_daily_$(date +\%Y\%m\%d).dump

# Weekly: Every Sunday at 3 AM UTC
0 3 * * 0 pg_dump -Fc mhc > /backups/mhc_weekly_$(date +\%Y_W\%V).dump

# Verification: Every day at 4 AM UTC
0 4 * * * pg_restore --list /backups/mhc_daily_$(date +\%Y\%m\%d).dump > /dev/null && echo "‚úÖ Backup valid"
```

### Media Backups (MinIO + ZFS)

```bash
# Nightly cold storage sync at 3 AM UTC
0 3 * * * rsync -av --delete /uploads /coldstorage/uploads

# ZFS snapshots (every 6 hours)
0 */6 * * * zfs snapshot pool/mhc@backup_$(date +\%Y\%m\%d_\%H\%M)

# ZFS retention: Keep 14 days of snapshots
0 5 * * * for snapshot in $(zfs list -t snapshot -o name | grep mhc@backup); do
  SNAP_DATE=$(date -d "$(echo $snapshot | cut -d_ -f2-)" +%s)
  if [ $(($(date +%s) - SNAP_DATE)) -gt 1209600 ]; then
    zfs destroy $snapshot
  fi
done
```

### WAL Archive (Point-in-Time Recovery)

```bash
# PostgreSQL archive_command in postgresql.conf
archive_command = 'test ! -f /wal_archive/%f && cp %p /wal_archive/%f'
archive_timeout = 300  # Archive every 5 minutes

# Backup WAL archives weekly
0 4 * * 0 tar -czf /backups/wal_archive_$(date +\%Y_W\%V).tar.gz /wal_archive/
```

---

## üö® DISASTER DETECTION & AUTO-FAILOVER

### Health Check Service

```typescript
// src/services/disaster-recovery.ts

import axios from 'axios';
import { prisma } from '../prisma';

export class DisasterRecoveryMonitor {
  private checkInterval = 30000; // Every 30 seconds

  async startMonitoring() {
    setInterval(() => this.runHealthChecks(), this.checkInterval);
  }

  async runHealthChecks() {
    try {
      // Check database connectivity
      await prisma.$queryRaw`SELECT 1`;
      
      // Check Redis
      const redis = require('redis').createClient();
      await redis.ping();
      
      // Check disk space
      const diskUsage = await this.getDiskUsage();
      if (diskUsage > 90) {
        await this.alertOps('CRITICAL: Disk usage >90%');
      }

      // Check backup freshness
      const lastBackup = await this.getLastBackupTime();
      if (Date.now() - lastBackup > 3600000) { // > 1 hour
        await this.alertOps('WARNING: Backup stale (>1 hour old)');
      }

      // Check replication lag
      const lag = await this.getReplicationLag();
      if (lag > 60000) { // > 1 minute
        await this.alertOps(`WARNING: Replication lag ${lag}ms`);
      }
    } catch (error) {
      await this.triggerFailover();
    }
  }

  async triggerFailover() {
    console.error('FAILOVER TRIGGERED: Promoting warm standby');
    
    // 1. Notify ops team
    await this.alertOps('CRITICAL: Failover in progress');
    
    // 2. Promote secondary PostgreSQL
    await this.promoteSecondaryDB();
    
    // 3. Update DNS to point to secondary
    await this.updateDNSFailover();
    
    // 4. Start Big Tech Shutdown Mode if needed
    await this.evaluateShutdownMode();
    
    // 5. Log to forensic trail
    await prisma.forensicLog.create({
      data: {
        eventType: 'SYSTEM_FAILOVER',
        entity: 'infrastructure',
        entityId: 'primary',
        payload: { event: 'failover_triggered', timestamp: new Date() },
        hash: 'FAILOVER_EVENT',
        prevHash: 'FAILOVER_EVENT'
      }
    });
  }

  private async promoteSecondaryDB() {
    // Connect to secondary DB
    // Execute: SELECT pg_promote();
    // Wait for promotion to complete
  }

  private async updateDNSFailover() {
    // Update DNS record to point to secondary IP
    // TTL: 60 seconds for quick failover
  }

  private async evaluateShutdownMode() {
    // Check if external services are reachable
    // If not, enable Big Tech Shutdown Mode
  }

  private async getDiskUsage(): Promise<number> {
    const fs = require('fs');
    // Implementation to check disk usage
    return 0;
  }

  private async getLastBackupTime(): Promise<number> {
    // Check /backups directory for latest file
    return Date.now();
  }

  private async getReplicationLag(): Promise<number> {
    // Query pg_stat_replication
    return 0;
  }

  private async alertOps(message: string) {
    console.error(`[DISASTER ALERT] ${message}`);
    // Send to Slack/PagerDuty
  }
}
```

---

## üîå BIG TECH SHUTDOWN MODE (Automatic Activation)

When external services fail, this mode activates automatically:

### Payments (No Stripe)

```typescript
// Switch to manual payment methods
const PAYMENT_METHODS = {
  crypto: {
    wallets: ['0x...', '0x...'],  // Ethereum addresses
    instructions: 'Send to one of these wallets'
  },
  bank_wire: {
    account: 'IBAN-xxx',
    instructions: 'Wire transfer details'
  },
  manual_invoice: {
    instructions: 'Contact support@mhc.com for invoice'
  }
};

// Accept payment without Stripe
export async function acceptPayment(method: keyof typeof PAYMENT_METHODS, amount: number) {
  if (method === 'crypto') {
    return `Send ${amount} USD equivalent to ${PAYMENT_METHODS.crypto.wallets[0]}`;
  }
  
  // Create manual invoice
  await prisma.invoice.create({
    data: { amount, status: 'unpaid', method }
  });
}
```

### Livestream (No External CDN)

```nginx
# NGINX-RTMP fallback configuration
events {
  worker_connections 1024;
}

http {
  server {
    listen 80;
    server_name stream.mhc.local;
    
    location /live {
      proxy_pass http://localhost:1935;
    }
  }
}

rtmp {
  server {
    listen 1935;
    chunk_size 4096;
    
    application live {
      live on;
      record off;
      
      # HLS output for fallback CDN
      hls on;
      hls_path /tmp/hls;
      hls_fragment 10s;
      hls_playlist_length 30s;
    }
  }
}
```

### Chat (No Firebase)

```typescript
// NATS-based chat fallback
import { connect } from 'nats';

const nc = await connect({ servers: ['localhost:4222'] });

// Publish chat message
async function broadcastMessage(streamId: string, message: string) {
  const msg = {
    streamId,
    message,
    timestamp: new Date(),
    sender: 'system'
  };
  
  nc.publish(`streams.${streamId}.chat`, JSON.stringify(msg));
}

// Subscribe to stream messages
const sub = nc.subscribe(`streams.${streamId}.chat`);
for await (const m of sub) {
  const msg = JSON.parse(new TextDecoder().decode(m.data));
  // Broadcast to all connected clients
}
```

### DNS (Hardcoded Fallback)

```typescript
// If DNS fails, use hardcoded IP
const PRIMARY_IP = '192.168.1.100';  // Primary datacenter
const SECONDARY_IP = '192.168.1.101'; // Warm standby

export async function resolveHost(hostname: string): Promise<string> {
  try {
    // Try normal DNS
    const { lookup } = require('dns/promises');
    return await lookup(hostname);
  } catch {
    // Fallback to hardcoded IPs
    if (hostname === 'api.mhc.com') return PRIMARY_IP;
    if (hostname === 'cdn.mhc.com') return SECONDARY_IP;
    throw new Error(`Unknown host: ${hostname}`);
  }
}
```

---

## üõ°Ô∏è OFFLINE-FIRST MOBILE ARCHITECTURE

Mobile app continues working even with no internet:

```typescript
// src/services/offline.service.ts (React Native)

import AsyncStorage from '@react-native-async-storage/async-storage';

export class OfflineService {
  // Cache content locally
  async cacheVideo(video: Video) {
    const cached = {
      id: video.id,
      title: video.title,
      url: video.url,
      metadata: video.metadata,
      downloaded: new Date(),
    };
    
    await AsyncStorage.setItem(`video_${video.id}`, JSON.stringify(cached));
  }

  // Serve from local cache
  async getVideo(videoId: string): Promise<Video | null> {
    try {
      const cached = await AsyncStorage.getItem(`video_${videoId}`);
      if (cached) return JSON.parse(cached);
    } catch {}
    
    return null;
  }

  // Queue actions for sync when online
  async queueAction(action: PendingAction) {
    const queue = await AsyncStorage.getItem('pending_actions') || '[]';
    const actions = JSON.parse(queue);
    actions.push({ ...action, timestamp: Date.now() });
    await AsyncStorage.setItem('pending_actions', JSON.stringify(actions));
  }

  // Sync when back online
  async syncPending() {
    const queue = await AsyncStorage.getItem('pending_actions') || '[]';
    const actions = JSON.parse(queue);
    
    for (const action of actions) {
      try {
        await this.executeAction(action);
        // Remove from queue
      } catch {
        // Keep in queue, retry later
      }
    }
  }

  // Peer-to-peer sharing
  async shareViaPeerMesh(videoId: string, peers: string[]) {
    // Use Bluetooth/WiFi Direct to share cached content
    for (const peer of peers) {
      await this.sendToPeer(peer, videoId);
    }
  }
}
```

---

## üìã DISASTER RECOVERY RUNBOOK

### Scenario 1: Primary Datacenter Offline (< 1 hour RTO)

```
1. ‚úÖ Health check fails ‚Üí Failover triggered automatically
2. ‚úÖ Secondary DB promoted via pg_promote()
3. ‚úÖ DNS updated to secondary IP (60-second TTL)
4. ‚úÖ All traffic routed to warm standby
5. ‚úÖ Monitor replication lag
6. ‚úÖ Contact ops team via PagerDuty
7. ‚úÖ Once primary recovers, sync and failback
8. ‚úÖ Log incident in forensic trail
```

**Time to recovery**: 2-5 minutes (automatic)

### Scenario 2: Extended Outage (> 6 hours)

```
1. ‚úÖ Enable Big Tech Shutdown Mode
2. ‚úÖ Switch payments to crypto/bank transfer
3. ‚úÖ Switch livestreaming to NGINX-RTMP
4. ‚úÖ Switch chat to NATS
5. ‚úÖ Switch DNS to hardcoded IPs
6. ‚úÖ Distribute content via local CDN cache
7. ‚úÖ Mobile users: Offline mode + mesh sharing
8. ‚úÖ Continue accepting payments manually
9. ‚úÖ Log all transactions for sync when online
10. ‚úÖ Monitor and wait for service restoration
```

**Time to partial recovery**: 30 minutes
**Time to full recovery**: 2-4 hours after services restored

### Scenario 3: Data Corruption / Ransomware

```
1. ‚úÖ Detect corruption via daily backup verification
2. ‚úÖ STOP writing to primary database immediately
3. ‚úÖ Failover to warm standby (unaffected)
4. ‚úÖ Restore from clean backup:
   - Last hourly backup if < 1 hour old
   - Last daily backup if < 24 hours
   - Last weekly backup as last resort
5. ‚úÖ Verify data integrity
6. ‚úÖ Test in isolated environment first
7. ‚úÖ Promote restored version to production
8. ‚úÖ Investigate root cause
9. ‚úÖ Document incident in forensic trail
```

**Time to recovery**: 30 minutes to 2 hours

---

## üîê BACKUP VERIFICATION

Every backup is tested daily:

```bash
#!/bin/bash
# test-backup.sh - Run daily at 4 AM UTC

BACKUP_FILE="/backups/mhc_daily_$(date +%Y%m%d).dump"

if [ ! -f "$BACKUP_FILE" ]; then
  echo "‚ùå CRITICAL: Backup file not found!"
  exit 1
fi

# Test restore to temporary database
echo "Testing backup restore..."
createdb mhc_test
pg_restore --no-acl --no-owner -d mhc_test "$BACKUP_FILE"

# Verify data integrity
echo "Verifying data integrity..."
psql mhc_test -c "SELECT COUNT(*) FROM users;" > /dev/null
psql mhc_test -c "SELECT COUNT(*) FROM videos;" > /dev/null

# Drop test database
dropdb mhc_test

echo "‚úÖ Backup verified successfully"
```

---

## üìä MONITORING & ALERTING

### Prometheus Metrics

```yaml
# prometheus.yml
scrape_configs:
  - job_name: 'mhc-backend'
    static_configs:
      - targets: ['localhost:9090']

  - job_name: 'postgresql'
    static_configs:
      - targets: ['localhost:9187']

  - job_name: 'minio'
    static_configs:
      - targets: ['localhost:9323']
```

### Alert Rules

```yaml
# alerts.yml
groups:
  - name: mhc_critical
    rules:
      # Database offline
      - alert: PostgreSQLDown
        expr: pg_up == 0
        for: 1m
        
      # Replication lag > 1 minute
      - alert: HighReplicationLag
        expr: pg_replication_slot_restart_lsn_bytes_lag > 1000000
        
      # Disk usage > 90%
      - alert: HighDiskUsage
        expr: (1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes)) > 0.9
        
      # Backup stale (> 1 hour)
      - alert: StaleBackup
        expr: (time() - backup_mtime_seconds) > 3600
```

---

## ‚úÖ DISASTER RECOVERY CHECKLIST

Before going to production:

- [ ] Primary and secondary datacenters identified
- [ ] PostgreSQL replication configured and tested
- [ ] Automatic backups running and verified
- [ ] WAL archives enabled and rotating
- [ ] Health check service monitoring all systems
- [ ] Failover process tested monthly
- [ ] DNS failover configured (60-second TTL)
- [ ] Big Tech Shutdown Mode code reviewed
- [ ] NGINX-RTMP fallback tested
- [ ] NATS chat fallback deployed
- [ ] Mobile offline mode implemented
- [ ] P2P mesh sharing implemented
- [ ] Hardcoded DNS fallback configured
- [ ] Forensic logging on all failover events
- [ ] Ops team trained on runbook
- [ ] Incident response plan documented
- [ ] RTO/RPO targets: < 5 minutes RTO, < 15 minutes RPO
- [ ] Annual disaster recovery drill completed

---

## üéØ PHILOSOPHY

> **MHC must survive anything. Internet goes down? We work. Stripe fails? We take manual payments. External APIs unreachable? We switch to self-hosted alternatives. Always available. Always online. Always for the artists.**

---

**Status**: ‚úÖ PRODUCTION-READY
**Last Tested**: [Update after monthly drill]
**Next Drill**: [Schedule quarterly]
**Responsible**: DevOps Team
